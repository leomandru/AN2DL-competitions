{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2_Final.ipynb","provenance":[],"authorship_tag":"ABX9TyON808OvLvrkOs2x9zFKiB/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tir9zt5L3o9e"},"source":["# UTILITIES"]},{"cell_type":"markdown","metadata":{"id":"vY3wh6g53rx4"},"source":["## SHELL OUTPUT"]},{"cell_type":"code","metadata":{"id":"CNueIj-9Cwd9"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBsMtm5g3uFJ"},"source":["## MODULES IMPORT"]},{"cell_type":"code","metadata":{"id":"gA1Lp4BbEREV"},"source":["import os\n","\n","import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6d7zoN_H3y1Q"},"source":["## UNZIP DATA"]},{"cell_type":"code","metadata":{"id":"BqFDMcbpLEzi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635700460616,"user_tz":-60,"elapsed":154796,"user":{"displayName":"Leonardo Mandruzzato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpP_4w-YyRfoPLwYpen1DWyZNKd5CWI8L-DKegq3g=s64","userId":"04202313098734066302"}},"outputId":"71fadae3-0e5a-4e35-8268-3fef2f038ff4"},"source":["!wget \"https://competitions.codalab.org/my/datasets/download/29a85805-2d8d-4701-a9ab-295180c89eb3\"\n","!unzip -q /content/29a85805-2d8d-4701-a9ab-295180c89eb3"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-31 17:10:22--  https://competitions.codalab.org/my/datasets/download/29a85805-2d8d-4701-a9ab-295180c89eb3\n","Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n","Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: https://minioec-proxy.lri.fr/prod-private/dataset_data_file/None/0cf1d/Development_Dataset.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=258e2c57f76a03d32ac15df83e7d608a2fd29c782b97d56ba100a417a4a922f4&X-Amz-Date=20211031T171023Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20211031%2Fminioec-proxy%2Fs3%2Faws4_request [following]\n","--2021-10-31 17:10:23--  https://minioec-proxy.lri.fr/prod-private/dataset_data_file/None/0cf1d/Development_Dataset.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=258e2c57f76a03d32ac15df83e7d608a2fd29c782b97d56ba100a417a4a922f4&X-Amz-Date=20211031T171023Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20211031%2Fminioec-proxy%2Fs3%2Faws4_request\n","Resolving minioec-proxy.lri.fr (minioec-proxy.lri.fr)... 129.175.15.21\n","Connecting to minioec-proxy.lri.fr (minioec-proxy.lri.fr)|129.175.15.21|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3501596229 (3.3G) [application/zip]\n","Saving to: ‘29a85805-2d8d-4701-a9ab-295180c89eb3’\n","\n","29a85805-2d8d-4701- 100%[===================>]   3.26G  24.9MB/s    in 2m 59s  \n","\n","2021-10-31 17:13:23 (18.7 MB/s) - ‘29a85805-2d8d-4701-a9ab-295180c89eb3’ saved [3501596229/3501596229]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"OYf0_yXq9qRO"},"source":["# DIRECTORIES DEFINITION"]},{"cell_type":"markdown","metadata":{"id":"2ixJuxZt9dnw"},"source":["## CROPPING FUNCTION"]},{"cell_type":"code","metadata":{"id":"BPA89FdF1p7Q"},"source":["import cv2\n","\n","def img_crop(img_source, img_dest, mask_source, mask_dest, name, frmt):\n","\n","  \"\"\"\n","  This function crops a specific image into 256x256 pieces. If the image is not\n","  divisible by 256, it will create some images with duplicated portions.\n","\n","  \"\"\"\n","\n","  DIM = 256\n","\n","  update_last_row = False\n","  update_last_column = False\n","\n","  img_arr = cv2.imread(img_source)\n","  mask_arr = cv2.imread(mask_source)\n","\n","  if (img_arr.shape[0]%DIM):\n","    update_last_row = True\n","  if (img_arr.shape[1]%DIM):\n","    update_last_column = True\n","\n","  # default crops\n","  for l in range(img_arr.shape[0]//DIM):\n","    for c in range(img_arr.shape[1]//DIM):\n","      # image crop & save\n","      crop_img_arr = img_arr[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(img_dest, name + \"_\" + str(l) + \"_\" + str(c) + frmt), crop_img_arr)\n","      # mask crop & save\n","      crop_mask_arr = mask_arr[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(mask_dest, name + \"_\" + str(l) + \"_\" + str(c) + '.png'), crop_mask_arr)\n","  \n","  # last smaller column crop (there will be some duplicated portions of images)\n","  if update_last_column:\n","    for l in range(img_arr.shape[0]//DIM):\n","      # image crop & save\n","      crop_img_arr = img_arr[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]]\n","      cv2.imwrite(os.path.join(img_dest, name + \"_\" + str(l) + \"_last\" + frmt), crop_img_arr)\n","      # mask crop & save\n","      crop_mask_arr = mask_arr[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]]\n","      cv2.imwrite(os.path.join(mask_dest, name + \"_\" + str(l) + \"_last\" + '.png'), crop_mask_arr)\n","\n","  # last smaller row crop (there will be some duplicated portions of images)\n","  if update_last_row:\n","    for c in range(img_arr.shape[1]//DIM):\n","      # image crop & save\n","      crop_img_arr = img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(img_dest, name + \"_last\" + \"_\" + str(c) + frmt), crop_img_arr)\n","      # mask crop & save\n","      crop_mask_arr = mask_arr[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(mask_dest, name + \"_last\" + \"_\" + str(c) + '.png'), crop_mask_arr)\n","\n","  # remaining portion crop (there will be some duplicated portions of images)\n","  if update_last_column and update_last_row:\n","    # image crop & save\n","    crop_img_arr = img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]]\n","    cv2.imwrite(os.path.join(img_dest, name + \"_last_last\" + frmt), crop_img_arr)\n","    # mask crop & save\n","    crop_mask_arr = mask_arr[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]]\n","    cv2.imwrite(os.path.join(mask_dest, name + \"_last_last\" + '.png'), crop_mask_arr)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8KudMc6lPSJH"},"source":["## FINAL DATASET\n","This script will create a folder called \"**FinalDataset**\" and a sub-folder called \"training\"."]},{"cell_type":"markdown","metadata":{"id":"KSW1nLJB6HUE"},"source":["### training dir\n","\n","Inside \"**training**\" there will be again 2 sub-folders: \"images\" and \"masks\". The first one will contain all the four-team images depicting both mais and haricot. On the other side, the second sub-directory will contain the respective masks of the pictures present in \"images\"."]},{"cell_type":"code","metadata":{"id":"n_nyHC5xRLg-"},"source":["import shutil\n","\n","dataset_dir = '/content/FinalDataset'\n","training_dir = os.path.join(dataset_dir, 'training')\n","\n","if not os.path.exists(dataset_dir):\n","  \n","  # FinalDataset\n","  os.makedirs(dataset_dir)\n","\n","  # training\n","  os.makedirs(training_dir)\n","  # images & masks\n","  os.makedirs(os.path.join(training_dir, 'images'))\n","  os.makedirs(os.path.join(training_dir, 'masks'))\n","\n","  starting_training_dir = '/content/Development_Dataset/Training'\n","  for team in os.listdir(starting_training_dir):\n","    for crop in os.listdir(os.path.join(starting_training_dir, team)):\n","      for f in os.listdir(os.path.join(starting_training_dir, team, crop, 'Images')):\n","        img_crop(img_source=os.path.join(starting_training_dir, team, crop, 'Images', f),\n","                 img_dest=os.path.join(training_dir, 'images'),\n","                 mask_source=os.path.join(starting_training_dir, team, crop, 'Masks', f[:-4] + '.png'),\n","                 mask_dest=os.path.join(training_dir, 'masks'),\n","                 name=f[:-4],\n","                 frmt=f[-4:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjKJGKL3hzOz"},"source":["### validation dir\n","This script will divide the whole content of \"/content/FinalDataset/training/images\" into 2 parts. The 80% of images into \"training\" and the 20% into \"**validation**\"."]},{"cell_type":"code","metadata":{"id":"m2KiZGl7iXE9"},"source":["import random, itertools \n","\n","validation_dir = '/content/FinalDataset/validation'\n","\n","if not os.path.exists(validation_dir):\n","  os.makedirs(validation_dir)\n","  os.makedirs(os.path.join(validation_dir, 'images'))\n","  os.makedirs(os.path.join(validation_dir, 'masks'))\n","else:\n","  # if already present the validation dir, this script will shuffle its content. \n","  # It will moove all the elements again to the \"training\" dir and after a shuffle \n","  # it will take 20% of the content and put ot in the \"validation\" dir.\n","  for f in os.listdir(os.path.join(validation_dir, 'images')):\n","      o = shutil.move(os.path.join(validation_dir, 'images', f),\n","                      os.path.join(training_dir, 'images', f))\n","      o = shutil.move(os.path.join(validation_dir, 'masks', f[:-3] + 'png'),\n","                      os.path.join(training_dir, 'masks', f[:-3] + 'png'))\n","      \n","    \n","images = os.listdir(os.path.join(training_dir, 'images'))\n","random.shuffle(images)\n","for f in itertools.islice(images, 0, int(0.2*len(images))):\n","  o = shutil.move(os.path.join(training_dir, 'images', f),\n","                  os.path.join(validation_dir, 'images', f))\n","  o = shutil.move(os.path.join(training_dir, 'masks', f[:-3] + 'png'),\n","                  os.path.join(validation_dir, 'masks', f[:-3] + 'png'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7M50ZVsH40w2"},"source":["# GENERATORS"]},{"cell_type":"markdown","metadata":{"id":"FH6U4tVz43O8"},"source":["## IMAGE DATA GENERATOR"]},{"cell_type":"code","metadata":{"id":"0E9-cyNOEa1p"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","apply_data_augmentation = True\n","\n","if apply_data_augmentation:\n","    img_data_gen = ImageDataGenerator(rotation_range=30,\n","                                      width_shift_range=10,\n","                                      height_shift_range=10,\n","                                      zoom_range=0.3,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect')\n","    mask_data_gen = ImageDataGenerator(rotation_range=30,\n","                                       width_shift_range=10,\n","                                       height_shift_range=10,\n","                                       zoom_range=0.3,\n","                                       horizontal_flip=True,\n","                                       vertical_flip=True,\n","                                       fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3NttKxjSH2J"},"source":["## CUSTOM DATASET"]},{"cell_type":"code","metadata":{"id":"hsieZk4aKhm6"},"source":["from PIL import Image\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","\n","  \"\"\"\n","    CustomDataset inheriting from tf.keras.utils.Sequence.\n","\n","    3 main methods:\n","      - __init__: save dataset params like directory, filenames..\n","      - __len__: return the total number of samples in the dataset\n","      - __getitem__: return a sample from the dataset\n","\n","    Note: \n","      - the custom dataset return a single sample from the dataset. Then, we use \n","        a tf.data.Dataset object to group samples into batches.\n","      - in this case we have a different structure of the dataset in memory. \n","        We have all the images in the same folder and the training and validation splits\n","        are defined in text files.\n","\n","  \"\"\"\n","\n","  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n","               preprocessing_function=None, out_shape=[256, 256]):\n","    if which_subset == 'training':\n","      subset_path = os.path.join(dataset_dir, 'training/images')\n","    elif which_subset == 'validation':\n","      subset_path = os.path.join(dataset_dir, 'validation/images')\n","\n","    subset_filenames = []\n","    for f in os.listdir(subset_path):\n","      subset_filenames.append(f.strip())\n","\n","    self.which_subset = which_subset\n","    self.dataset_dir = dataset_dir\n","    self.subset_filenames = subset_filenames\n","    self.img_generator = img_generator\n","    self.mask_generator = mask_generator\n","    self.preprocessing_function = preprocessing_function\n","    self.out_shape = out_shape\n","\n","  def __len__(self):\n","    return len(self.subset_filenames)\n","\n","  def __getitem__(self, index):\n","    # Read Image\n","    curr_filename = self.subset_filenames[index]\n","    img = Image.open(os.path.join(self.dataset_dir, self.which_subset, 'images', curr_filename))\n","    mask = Image.open(os.path.join(self.dataset_dir, self.which_subset, 'masks', curr_filename[:-4] + '.png'))\n","    \n","    img_arr = np.array(img)\n","\n","\n","    # START : assigned script for obtaining the numpy array containing target values of the masks\n","\n","    tmp_mask_arr = np.array(mask)\n","\n","    mask_arr = np.zeros(tmp_mask_arr.shape[:2], dtype=tmp_mask_arr.dtype)\n","    mask_arr[np.where(np.all(tmp_mask_arr == [216, 124, 18], axis=-1))] = 0\n","    mask_arr[np.where(np.all(tmp_mask_arr == [255, 255, 255], axis=-1))] = 1\n","    mask_arr[np.where(np.all(tmp_mask_arr == [216, 67, 82], axis=-1))] = 2\n","\n","    # END :  assigned script for obtaining the numpy array containing target values of the masks\n","\n","    mask_arr = np.expand_dims(mask_arr, -1)\n","\n","    if self.which_subset == 'training':\n","      if self.img_generator is not None and self.mask_generator is not None:\n","        # Perform data augmentation\n","        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n","        # and we can apply it to the image using apply_transform\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n","        # Thus, when applied to the masks it will output 'interpolated classes', which\n","        # is an unwanted behaviour. As a trick, we can transform each class mask \n","        # separately and then we can cast to integer values (as in the binary segmentation notebook).\n","        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n","        out_mask = np.zeros_like(mask_arr)\n","        for c in np.unique(mask_arr): # unique restituisce un array contenente solamente una copia per ogni differente valore presente nell'array\n","          if c > 0:\n","            curr_class_arr = np.float32(mask_arr == c)\n","            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n","            # from [0, 1] to {0, 1}\n","            curr_class_arr = np.uint8(curr_class_arr)\n","            # recover original class\n","            curr_class_arr = curr_class_arr * c \n","            out_mask += curr_class_arr\n","    else:\n","      out_mask = mask_arr\n","    \n","    if self.preprocessing_function is not None:\n","        img_arr = self.preprocessing_function(img_arr)\n","\n","    return img_arr, np.float32(out_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TyrdiIh_PWjB"},"source":["dataset = CustomDataset('/content/FinalDataset', \n","                        'training', \n","                        img_generator=img_data_gen, \n","                        mask_generator=mask_data_gen\n","                        )\n","dataset_valid = CustomDataset('/content/FinalDataset', \n","                              'validation'\n","                              )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUCPGq_6SSHm"},"source":["## DATASET DEFINITION"]},{"cell_type":"code","metadata":{"id":"5j9WRj0zL66c"},"source":["bs = 64\n","\n","img_h = 256\n","img_w = 256\n","\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n","train_dataset = train_dataset.batch(bs)\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n","valid_dataset = valid_dataset.batch(bs)\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NxcCZQeSbhn"},"source":["## DATA GENERATOR TEST"]},{"cell_type":"code","metadata":{"id":"bHAj3I8BRo3k"},"source":["iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WRxgjDvnQQd"},"source":["import time\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","# Assign a color to each class\n","colors_dict = {}\n","colors_dict[1] = [255, 255, 255]  # crop\n","colors_dict[0] = [0, 0, 0]  # background\n","colors_dict[2] = [216, 67, 82] # weed\n","\n","fig, ax = plt.subplots(1, 2)\n","\n","augmented_img, target = next(iterator)\n","augmented_img = augmented_img[0]   # First element\n","augmented_img = augmented_img  # denormalize\n","augmented_img.shape\n","\n","target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n","\n","print(np.unique(target))\n","\n","# Assign colors (just for visualization)\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\n","\n","target_img[np.where(target == 0)] = colors_dict[0]\n","target_img[np.where(target == 1)] = colors_dict[1]\n","target_img[np.where(target == 2)] = colors_dict[2]\n","\n","ax[0].imshow(np.uint8(augmented_img))\n","ax[1].imshow(np.uint8(target_img))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ulx88lVm5sla"},"source":["# MODEL DEFINITION"]},{"cell_type":"code","metadata":{"id":"NWQ8vCplLLFI"},"source":["from tensorflow.keras import layers\n","\n","def get_model(img_size, num_classes):\n","    inputs = tf.keras.Input(shape=img_size + (3,))\n","\n","    ### [First half of the network: downsampling inputs] ###\n","\n","    # Entry block\n","    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    previous_block_activation = x  # Set aside residual\n","\n","    # Blocks 1, 2, 3 are identical apart from the feature depth.\n","    for filters in [64, 128, 256]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n","\n","        # Project residual\n","        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n","            previous_block_activation\n","        )\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    ### [Second half of the network: upsampling inputs] ###\n","\n","    for filters in [256, 128, 64, 32]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.UpSampling2D(2)(x)\n","\n","        # Project residual\n","        residual = layers.UpSampling2D(2)(previous_block_activation)\n","        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    # Add a per-pixel classification layer\n","    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n","\n","    # Define the model\n","    model = tf.keras.Model(inputs, outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsF7WDJM8DCM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635370381426,"user_tz":-120,"elapsed":880,"user":{"displayName":"Leonardo Mandruzzato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpP_4w-YyRfoPLwYpen1DWyZNKd5CWI8L-DKegq3g=s64","userId":"04202313098734066302"}},"outputId":"f89faca2-e194-4644-d711-a5176329f4f4"},"source":["model = get_model((img_h, img_w), 3)\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 128, 128, 32) 896         input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 128, 128, 32) 128         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 128, 128, 32) 0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 128, 128, 32) 0           activation_15[0][0]              \n","__________________________________________________________________________________________________\n","separable_conv2d_6 (SeparableCo (None, 128, 128, 64) 2400        activation_16[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_6[0][0]         \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 128, 128, 64) 0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_7 (SeparableCo (None, 128, 128, 64) 4736        activation_17[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_7[0][0]         \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 64)   0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 64, 64, 64)   2112        activation_15[0][0]              \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 64, 64, 64)   0           max_pooling2d_3[0][0]            \n","                                                                 conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 64, 64, 64)   0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_8 (SeparableCo (None, 64, 64, 128)  8896        activation_18[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_8[0][0]         \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_9 (SeparableCo (None, 64, 64, 128)  17664       activation_19[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_9[0][0]         \n","__________________________________________________________________________________________________\n","max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 128)  0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 32, 128)  8320        add_7[0][0]                      \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 32, 32, 128)  0           max_pooling2d_4[0][0]            \n","                                                                 conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 32, 32, 128)  0           add_8[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_10 (SeparableC (None, 32, 32, 256)  34176       activation_20[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_10[0][0]        \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 32, 32, 256)  0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_11 (SeparableC (None, 32, 32, 256)  68096       activation_21[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_11[0][0]        \n","__________________________________________________________________________________________________\n","max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 256)  0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 16, 16, 256)  33024       add_8[0][0]                      \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 16, 16, 256)  0           max_pooling2d_5[0][0]            \n","                                                                 conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 16, 16, 256)  0           add_9[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_transpose_8 (Conv2DTrans (None, 16, 16, 256)  590080      activation_22[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 16, 16, 256)  1024        conv2d_transpose_8[0][0]         \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 16, 16, 256)  0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 256)  590080      activation_23[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 16, 16, 256)  1024        conv2d_transpose_9[0][0]         \n","__________________________________________________________________________________________________\n","up_sampling2d_9 (UpSampling2D)  (None, 32, 32, 256)  0           add_9[0][0]                      \n","__________________________________________________________________________________________________\n","up_sampling2d_8 (UpSampling2D)  (None, 32, 32, 256)  0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 32, 32, 256)  65792       up_sampling2d_9[0][0]            \n","__________________________________________________________________________________________________\n","add_10 (Add)                    (None, 32, 32, 256)  0           up_sampling2d_8[0][0]            \n","                                                                 conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 32, 32, 256)  0           add_10[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_transpose_10 (Conv2DTran (None, 32, 32, 128)  295040      activation_24[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_transpose_10[0][0]        \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_11 (Conv2DTran (None, 32, 32, 128)  147584      activation_25[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_transpose_11[0][0]        \n","__________________________________________________________________________________________________\n","up_sampling2d_11 (UpSampling2D) (None, 64, 64, 256)  0           add_10[0][0]                     \n","__________________________________________________________________________________________________\n","up_sampling2d_10 (UpSampling2D) (None, 64, 64, 128)  0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 64, 64, 128)  32896       up_sampling2d_11[0][0]           \n","__________________________________________________________________________________________________\n","add_11 (Add)                    (None, 64, 64, 128)  0           up_sampling2d_10[0][0]           \n","                                                                 conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 64, 64, 128)  0           add_11[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_transpose_12 (Conv2DTran (None, 64, 64, 64)   73792       activation_26[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_12[0][0]        \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 64, 64, 64)   0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_13 (Conv2DTran (None, 64, 64, 64)   36928       activation_27[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_13[0][0]        \n","__________________________________________________________________________________________________\n","up_sampling2d_13 (UpSampling2D) (None, 128, 128, 128 0           add_11[0][0]                     \n","__________________________________________________________________________________________________\n","up_sampling2d_12 (UpSampling2D) (None, 128, 128, 64) 0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 128, 128, 64) 8256        up_sampling2d_13[0][0]           \n","__________________________________________________________________________________________________\n","add_12 (Add)                    (None, 128, 128, 64) 0           up_sampling2d_12[0][0]           \n","                                                                 conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 128, 128, 64) 0           add_12[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_transpose_14 (Conv2DTran (None, 128, 128, 32) 18464       activation_28[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 128, 128, 32) 128         conv2d_transpose_14[0][0]        \n","__________________________________________________________________________________________________\n","activation_29 (Activation)      (None, 128, 128, 32) 0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_15 (Conv2DTran (None, 128, 128, 32) 9248        activation_29[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 128, 128, 32) 128         conv2d_transpose_15[0][0]        \n","__________________________________________________________________________________________________\n","up_sampling2d_15 (UpSampling2D) (None, 256, 256, 64) 0           add_12[0][0]                     \n","__________________________________________________________________________________________________\n","up_sampling2d_14 (UpSampling2D) (None, 256, 256, 32) 0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 256, 256, 32) 2080        up_sampling2d_15[0][0]           \n","__________________________________________________________________________________________________\n","add_13 (Add)                    (None, 256, 256, 32) 0           up_sampling2d_14[0][0]           \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 256, 256, 3)  867         add_13[0][0]                     \n","==================================================================================================\n","Total params: 2,058,979\n","Trainable params: 2,055,203\n","Non-trainable params: 3,776\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"NGzh16WTnFJW"},"source":["# PARAMS"]},{"cell_type":"code","metadata":{"id":"9MlmYGVMnFJW"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Here we define the intersection over union for each class in the batch.\n","# Then we compute the final iou as the mean over classes\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","# Validation metrics\n","# ------------------\n","metrics = ['accuracy', meanIoU]\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUPlIkmg53Wo"},"source":["# CALLBACKS"]},{"cell_type":"code","metadata":{"id":"uxQp3voMq7Gn"},"source":["from datetime import datetime\n","\n","cwd = '/content/drive/MyDrive/AN2DL-competitions/HW2'\n","\n","experiments_dir = os.path.join(cwd, 'experiments')\n","if not os.path.exists(experiments_dir):\n","    os.makedirs(experiments_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","model_name = 'Proj2'\n","\n","proj_dir = os.path.join(experiments_dir, model_name + '_' + str(now))\n","if not os.path.exists(proj_dir):\n","    os.makedirs(proj_dir)\n","    \n","callbacks = []\n","\n","ckpt_dir = os.path.join(proj_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True) \n","callbacks.append(ckpt_callback)\n","\n","tb_dir = os.path.join(proj_dir, 'tb-logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)\n","callbacks.append(tb_callback)\n","\n","early_stop = False\n","if early_stop:\n","    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0MZCPu9Q543l"},"source":["# TRAINING"]},{"cell_type":"code","metadata":{"id":"Yvbr57uJsC8r"},"source":["model.fit(x=train_dataset,\n","          epochs=100,  #### set repeat in training dataset\n","          steps_per_epoch=len(dataset)//bs,\n","          validation_data=valid_dataset,\n","          validation_steps=len(dataset_valid)//bs, \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFccIHClgD6H"},"source":["# SUBMISSION"]},{"cell_type":"markdown","metadata":{"id":"-UBoZZcBgIMS"},"source":["## MODEL RELOADING"]},{"cell_type":"code","metadata":{"id":"7Q31i5NUOnwn"},"source":["# Da cambiare in base a quello che serve !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","model.load_weights(\"/content/drive/MyDrive/AN2DL_experiments_project_2/Proj2_Dec12_14-58-22/ckpts/cp_08.ckpt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzFbUfC8gLYX"},"source":["## RLE ENCODE SCRIPT"]},{"cell_type":"code","metadata":{"id":"Fyc1XbHgZJXp"},"source":["def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHtiMzTrgUz-"},"source":["## SUBMISSION JSON"]},{"cell_type":"code","metadata":{"id":"fmVxjkraM_R8"},"source":["def predict_cropped_image(cropped_img):\n","  out_sigmoid = model.predict(x=tf.expand_dims(cropped_img, 0))\n","  predicted_class = tf.argmax(out_sigmoid, -1)\n","  return np.squeeze(predicted_class.numpy())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SxFv1NC85W-"},"source":["import json\n","\n","testing_dir = '/content/Development_Dataset/Test_Dev'\n","submission_dict = {}\n","\n","DIM = 256\n","\n","for team in os.listdir(testing_dir):\n","  update_last_row = False\n","  update_last_column = False\n","  for crop in os.listdir(os.path.join(testing_dir, team)):\n","      for f in os.listdir(os.path.join(testing_dir, team, crop, 'Images')):\n","\n","        img_arr = cv2.imread(os.path.join(testing_dir, team, crop, 'Images', f))\n","        \n","        # necessary due to cv2.imread that creates a numpy array \n","        # of the BGR image and not of the RGB one\n","        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n","\n","        if not update_last_row and img_arr.shape[0]%DIM:\n","          update_last_row = True\n","        if not update_last_column and img_arr.shape[1]%DIM:\n","          update_last_column = True       \n","\n","        # predictions di default\n","        total_prediction = np.zeros(img_arr.shape[0:2])\n","        for l in range(img_arr.shape[0]//DIM):\n","          for c in range(img_arr.shape[1]//DIM):\n","            total_prediction[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM] = predict_cropped_image(img_arr[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM])\n","\n","        # predictions dell'ultima colonna\n","        if update_last_column:\n","          for l in range(img_arr.shape[0]//DIM):\n","            total_prediction[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]] = predict_cropped_image(img_arr[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]])\n","\n","        # predictions dell'ultima riga\n","        if update_last_row:\n","          for c in range(img_arr.shape[1]//DIM):\n","            total_prediction[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM] = predict_cropped_image(img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM])\n","\n","        # prediction cella rimanente\n","        if update_last_column and update_last_row:\n","          total_prediction[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]] = predict_cropped_image(img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]])\n","\n","        img_name = f[:-4]\n","\n","        submission_dict[img_name] = {}\n","        submission_dict[img_name]['shape'] = total_prediction.shape\n","        submission_dict[img_name]['team'] = team\n","        submission_dict[img_name]['crop'] = crop\n","        submission_dict[img_name]['segmentation'] = {}\n","\n","        # RLE encoding\n","        # crop\n","        rle_encoded_crop = rle_encode(total_prediction == 1)\n","        # weed\n","        rle_encoded_weed = rle_encode(total_prediction == 2)\n","\n","        submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","        submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","\n","with open(os.path.join('/content/submission.json'), 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}