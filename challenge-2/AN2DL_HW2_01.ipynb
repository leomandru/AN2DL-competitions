{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2_01.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPGg/KIdYNpBlXbEYFGOu3g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tir9zt5L3o9e"},"source":["# UTILITIES"]},{"cell_type":"markdown","metadata":{"id":"vY3wh6g53rx4"},"source":["## SHELL OUTPUT"]},{"cell_type":"code","metadata":{"id":"CNueIj-9Cwd9"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBsMtm5g3uFJ"},"source":["## MODULES IMPORT"]},{"cell_type":"code","metadata":{"id":"gA1Lp4BbEREV"},"source":["import os\n","\n","import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","import cv2\n","\n","SEED = 1234\n","tf.random.set_seed(SEED) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6d7zoN_H3y1Q"},"source":["## UNZIP DATA"]},{"cell_type":"code","metadata":{"id":"BqFDMcbpLEzi"},"source":["!wget \"https://competitions.codalab.org/my/datasets/download/29a85805-2d8d-4701-a9ab-295180c89eb3\"\n","!unzip -q /content/29a85805-2d8d-4701-a9ab-295180c89eb3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t4b-6ncy3-tB"},"source":["# DIRECTORIES DEFINITION"]},{"cell_type":"markdown","metadata":{"id":"8KudMc6lPSJH"},"source":["## FINAL DATASET\n","This script will create a folder called \"**FinalDataset**\" and a sub-folder called \"training\"."]},{"cell_type":"markdown","metadata":{"id":"KSW1nLJB6HUE"},"source":["### training dir\n","\n","Inside \"**training**\" there will be again 2 sub-folders: \"images\" and \"masks\". The first one will contain all the four-team images depicting both mais and haricot. On the other side, the second sub-directory will contain the respective masks of the pictures present in \"images\"."]},{"cell_type":"code","metadata":{"id":"n_nyHC5xRLg-"},"source":["import shutil\n","\n","dataset_dir = '/content/FinalDataset'\n","training_dir = os.path.join(dataset_dir, 'training')\n","\n","if not os.path.exists(dataset_dir):\n","  \n","  # FinalDataset\n","  os.makedirs(dataset_dir)\n","\n","  # training\n","  os.makedirs(training_dir)\n","  # images & masks\n","  os.makedirs(os.path.join(training_dir, 'images'))\n","  os.makedirs(os.path.join(training_dir, 'masks'))\n","\n","  starting_training_dir = '/content/Development_Dataset/Training'\n","  for team in os.listdir(starting_training_dir):\n","    for crop in os.listdir(os.path.join(starting_training_dir, team)):\n","      for f in os.listdir(os.path.join(starting_training_dir, team, crop, 'Images')):\n","        o = shutil.copy(os.path.join(starting_training_dir, team, crop, 'Images', f), \n","                        os.path.join(training_dir, 'images', f))\n","        o = shutil.copy(os.path.join(starting_training_dir, team, crop, 'Masks', f[:-4] + '.png'), \n","                        os.path.join(training_dir, 'masks', f[:-4] + '.png'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjKJGKL3hzOz"},"source":["### validation dir\n","This script will divide the whole content of \"/content/FinalDataset/training/images\" into 2 parts. The 80% of images into \"training\" and the 20% into \"**validation**\"."]},{"cell_type":"code","metadata":{"id":"m2KiZGl7iXE9"},"source":["import random, itertools \n","\n","validation_dir = '/content/FinalDataset/validation'\n","\n","if not os.path.exists(validation_dir):\n","  os.makedirs(validation_dir)\n","  os.makedirs(os.path.join(validation_dir, 'images'))\n","  os.makedirs(os.path.join(validation_dir, 'masks'))\n","else:\n","  # if already present the validation dir, this script will shuffle its content. \n","  # It will moove all the elements again to the \"training\" dir and after a shuffle \n","  # it will take 20% of the content and put ot in the \"validation\" dir.\n","  for f in os.listdir(os.path.join(validation_dir, 'images')):\n","      o = shutil.move(os.path.join(validation_dir, 'images', f),\n","                      os.path.join(training_dir, 'images', f))\n","      o = shutil.move(os.path.join(validation_dir, 'masks', f[:-3] + 'png'),\n","                      os.path.join(training_dir, 'masks', f[:-3] + 'png'))\n","      \n","    \n","images = os.listdir(os.path.join(training_dir, 'images'))\n","random.shuffle(images)\n","for f in itertools.islice(images, 0, int(0.2*len(images))):\n","  o = shutil.move(os.path.join(training_dir, 'images', f),\n","                  os.path.join(validation_dir, 'images', f))\n","  o = shutil.move(os.path.join(training_dir, 'masks', f[:-3] + 'png'),\n","                  os.path.join(validation_dir, 'masks', f[:-3] + 'png'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7M50ZVsH40w2"},"source":["# GENERATORS"]},{"cell_type":"markdown","metadata":{"id":"FH6U4tVz43O8"},"source":["## IMAGE DATA GENERATOR"]},{"cell_type":"code","metadata":{"id":"0E9-cyNOEa1p"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","apply_data_augmentation = True\n","\n","if apply_data_augmentation:\n","    img_data_gen = ImageDataGenerator(rotation_range=30,\n","                                      width_shift_range=10,\n","                                      height_shift_range=10,\n","                                      zoom_range=0.3,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect')\n","    mask_data_gen = ImageDataGenerator(rotation_range=30,\n","                                       width_shift_range=10,\n","                                       height_shift_range=10,\n","                                       zoom_range=0.3,\n","                                       horizontal_flip=True,\n","                                       vertical_flip=True,\n","                                       fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3NttKxjSH2J"},"source":["## CUSTOM DATASET"]},{"cell_type":"code","metadata":{"id":"hsieZk4aKhm6"},"source":["from PIL import Image\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","\n","  \"\"\"\n","    CustomDataset inheriting from tf.keras.utils.Sequence.\n","\n","    3 main methods:\n","      - __init__: save dataset params like directory, filenames..\n","      - __len__: return the total number of samples in the dataset\n","      - __getitem__: return a sample from the dataset\n","\n","    Note: \n","      - the custom dataset return a single sample from the dataset. Then, we use \n","        a tf.data.Dataset object to group samples into batches.\n","      - in this case we have a different structure of the dataset in memory. \n","        We have all the images in the same folder and the training and validation splits\n","        are defined in text files.\n","\n","  \"\"\"\n","\n","  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n","               preprocessing_function=None, out_shape=[256, 256]):\n","    if which_subset == 'training':\n","      subset_path = os.path.join(dataset_dir, 'training/images')\n","    elif which_subset == 'validation':\n","      subset_path = os.path.join(dataset_dir, 'validation/images')\n","\n","    subset_filenames = []\n","    for f in os.listdir(subset_path):\n","      subset_filenames.append(f.strip())\n","\n","    self.which_subset = which_subset\n","    self.dataset_dir = dataset_dir\n","    self.subset_filenames = subset_filenames\n","    self.img_generator = img_generator\n","    self.mask_generator = mask_generator\n","    self.preprocessing_function = preprocessing_function\n","    self.out_shape = out_shape\n","\n","  def __len__(self):\n","    return len(self.subset_filenames)\n","\n","  def __getitem__(self, index):\n","    # Read Image\n","    curr_filename = self.subset_filenames[index]\n","    img = Image.open(os.path.join(self.dataset_dir, self.which_subset, 'images', curr_filename))\n","    mask = Image.open(os.path.join(self.dataset_dir, self.which_subset, 'masks', curr_filename[:-4] + '.png'))\n","    \n","    img = img.resize(self.out_shape)\n","    mask = mask.resize(self.out_shape, resample=Image.NEAREST)\n","\n","    img_arr = np.array(img)\n","\n","\n","    # START : assigned script for obtaining the numpy array containing target values of the masks\n","\n","    tmp_mask_arr = np.array(mask)\n","\n","    mask_arr = np.zeros(tmp_mask_arr.shape[:2], dtype=tmp_mask_arr.dtype)\n","    mask_arr[np.where(np.all(tmp_mask_arr == [216, 124, 18], axis=-1))] = 0\n","    mask_arr[np.where(np.all(tmp_mask_arr == [255, 255, 255], axis=-1))] = 1\n","    mask_arr[np.where(np.all(tmp_mask_arr == [216, 67, 82], axis=-1))] = 2\n","\n","    # END :  assigned script for obtaining the numpy array containing target values of the masks\n","\n","    mask_arr = np.expand_dims(mask_arr, -1)\n","\n","    if self.which_subset == 'training':\n","      if self.img_generator is not None and self.mask_generator is not None:\n","        # Perform data augmentation\n","        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n","        # and we can apply it to the image using apply_transform\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n","        # Thus, when applied to the masks it will output 'interpolated classes', which\n","        # is an unwanted behaviour. As a trick, we can transform each class mask \n","        # separately and then we can cast to integer values (as in the binary segmentation notebook).\n","        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n","        out_mask = np.zeros_like(mask_arr)\n","        for c in np.unique(mask_arr): # unique restituisce un array contenente solamente una copia per ogni differente valore presente nell'array\n","          if c > 0:\n","            curr_class_arr = np.float32(mask_arr == c)\n","            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n","            # from [0, 1] to {0, 1}\n","            curr_class_arr = np.uint8(curr_class_arr)\n","            # recover original class\n","            curr_class_arr = curr_class_arr * c \n","            out_mask += curr_class_arr\n","    else:\n","      out_mask = mask_arr\n","    \n","    if self.preprocessing_function is not None:\n","        img_arr = self.preprocessing_function(img_arr)\n","\n","    return img_arr, np.float32(out_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TyrdiIh_PWjB"},"source":["dataset = CustomDataset('/content/FinalDataset', \n","                        'training', \n","                        img_generator=img_data_gen, \n","                        mask_generator=mask_data_gen\n","                        )\n","dataset_valid = CustomDataset('/content/FinalDataset', \n","                              'validation'\n","                              )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUCPGq_6SSHm"},"source":["## DATASET DEFINITION"]},{"cell_type":"code","metadata":{"id":"5j9WRj0zL66c"},"source":["bs = 16\n","\n","img_h = 256\n","img_w = 256\n","\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n","train_dataset = train_dataset.batch(bs)\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n","valid_dataset = valid_dataset.batch(bs)\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NxcCZQeSbhn"},"source":["## DATA GENERATOR TEST"]},{"cell_type":"code","metadata":{"id":"bHAj3I8BRo3k"},"source":["iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WRxgjDvnQQd"},"source":["import time\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","# Assign a color to each class\n","colors_dict = {}\n","colors_dict[1] = [255, 255, 255]  # crop\n","colors_dict[0] = [0, 0, 0]  # background\n","colors_dict[2] = [216, 67, 82] # weed\n","\n","fig, ax = plt.subplots(1, 2)\n","\n","augmented_img, target = next(iterator)\n","augmented_img = augmented_img[0]   # First element\n","augmented_img = augmented_img  # denormalize\n","augmented_img.shape\n","\n","target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n","\n","print(np.unique(target))\n","\n","# Assign colors (just for visualization)\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\n","\n","target_img[np.where(target == 0)] = colors_dict[0]\n","target_img[np.where(target == 1)] = colors_dict[1]\n","target_img[np.where(target == 2)] = colors_dict[2]\n","\n","ax[0].imshow(np.uint8(augmented_img))\n","ax[1].imshow(np.uint8(target_img))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ulx88lVm5sla"},"source":["# MODEL DEFINITION"]},{"cell_type":"code","metadata":{"id":"l8TvfNuvpt20"},"source":["img_h = 256\n","img_w = 256\n","\n","vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n","vgg.summary()\n","for layer in vgg.layers:\n","  layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3vSaJElpppn"},"source":["def create_model(depth, start_f, num_classes):\n","\n","    model = tf.keras.Sequential()\n","    \n","    # Encoder\n","    # -------\n","    model.add(vgg)\n","    \n","    start_f = 256\n","        \n","    # Decoder\n","    # -------\n","    for i in range(depth):\n","        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n","        model.add(tf.keras.layers.Conv2D(filters=start_f,\n","                                         kernel_size=(3, 3),\n","                                         strides=(1, 1),\n","                                         padding='same'))\n","        model.add(tf.keras.layers.ReLU())\n","\n","        start_f = start_f // 2\n","\n","    # Prediction Layer\n","    # ----------------\n","    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n","                                     kernel_size=(1, 1),\n","                                     strides=(1, 1),\n","                                     padding='same',\n","                                     activation='softmax'))\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tP0Yws_gp6zS"},"source":["model = create_model(depth=5, \n","                     start_f=8, \n","                     num_classes=3)\n","\n","# Visualize created model as a table\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGzh16WTnFJW"},"source":["# PARAMS"]},{"cell_type":"code","metadata":{"id":"9MlmYGVMnFJW"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Here we define the intersection over union for each class in the batch.\n","# Then we compute the final iou as the mean over classes\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","# Validation metrics\n","# ------------------\n","metrics = ['accuracy', meanIoU]\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUPlIkmg53Wo"},"source":["# CALLBACKS"]},{"cell_type":"code","metadata":{"id":"uxQp3voMq7Gn"},"source":["from datetime import datetime\n","\n","cwd = '/content/drive/MyDrive/'\n","\n","experiments_dir = os.path.join(cwd, 'AN2DL_experiments_project_2')\n","if not os.path.exists(experiments_dir):\n","    os.makedirs(experiments_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","model_name = 'Proj2'\n","\n","proj_dir = os.path.join(experiments_dir, model_name + '_' + str(now))\n","if not os.path.exists(proj_dir):\n","    os.makedirs(proj_dir)\n","    \n","callbacks = []\n","\n","ckpt_dir = os.path.join(proj_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True) \n","callbacks.append(ckpt_callback)\n","\n","tb_dir = os.path.join(proj_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)\n","callbacks.append(tb_callback)\n","\n","early_stop = False\n","if early_stop:\n","    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0MZCPu9Q543l"},"source":["# TRAINING"]},{"cell_type":"code","metadata":{"id":"Yvbr57uJsC8r"},"source":["model.fit(x=train_dataset,\n","          epochs=100,  #### set repeat in training dataset\n","          steps_per_epoch=len(dataset)//bs,\n","          validation_data=valid_dataset,\n","          validation_steps=len(dataset_valid)//bs, \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFccIHClgD6H"},"source":["# SUBMISSION"]},{"cell_type":"markdown","metadata":{"id":"-UBoZZcBgIMS"},"source":["## MODEL RELOADING"]},{"cell_type":"code","metadata":{"id":"7Q31i5NUOnwn"},"source":["model.load_weights(\"...\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzFbUfC8gLYX"},"source":["## RLE ENCODE SCRIPT"]},{"cell_type":"code","metadata":{"id":"Fyc1XbHgZJXp"},"source":["def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHtiMzTrgUz-"},"source":["## SUBMISSION JSON"]},{"cell_type":"code","metadata":{"id":"6SxFv1NC85W-"},"source":["import json\n","\n","testing_dir = '/content/Development_Dataset/Test_Dev'\n","submission_dict = {}\n","\n","w = 256\n","h = 256\n","\n","for team in os.listdir(testing_dir):\n","  for crop in os.listdir(os.path.join(testing_dir, team)):\n","      for f in os.listdir(os.path.join(testing_dir, team, crop, 'Images')):\n","\n","        img_arr = cv2.imread(os.path.join(testing_dir, team, crop, 'Images', f))\n","\n","        dim = img_arr.shape[:2]\n","        \n","        # necessary due to cv2.imread that creates a numpy array \n","        # of the BGR image and not of the RGB one\n","        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n","\n","        img_arr = cv2.resize(img_arr, dsize=(w, h), interpolation=cv2.INTER_CUBIC)\n","\n","        out_sigmoid = model.predict(x=tf.expand_dims(img_arr, 0))\n","        out_sigmoid = tf.image.resize(out_sigmoid, dim, method = 'nearest')\n","        predicted_class = tf.argmax(out_sigmoid, -1)\n","        predicted_class = np.squeeze(predicted_class.numpy())\n","        \n","        img_name = f[:-4]\n","\n","        submission_dict[img_name] = {}\n","        submission_dict[img_name]['shape'] = dim\n","        submission_dict[img_name]['team'] = team\n","        submission_dict[img_name]['crop'] = crop\n","        submission_dict[img_name]['segmentation'] = {}\n","\n","        # RLE encoding\n","        # crop\n","        rle_encoded_crop = rle_encode(total_prediction == 1)\n","        # weed\n","        rle_encoded_weed = rle_encode(total_prediction == 2)\n","\n","        submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","        submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","\n","with open(os.path.join('/content/submission.json'), 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}