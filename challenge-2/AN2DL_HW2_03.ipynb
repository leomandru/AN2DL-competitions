{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2_03.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMi7tDsaEac43km2aytuvES"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tir9zt5L3o9e"},"source":["# UTILITIES"]},{"cell_type":"markdown","metadata":{"id":"vY3wh6g53rx4"},"source":["## SHELL OUTPUT"]},{"cell_type":"code","metadata":{"id":"CNueIj-9Cwd9"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBsMtm5g3uFJ"},"source":["## MODULES IMPORT"]},{"cell_type":"code","metadata":{"id":"gA1Lp4BbEREV"},"source":["import os\n","\n","import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","\n","SEED = 1234\n","tf.random.set_seed(SEED) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6d7zoN_H3y1Q"},"source":["## UNZIP DATA"]},{"cell_type":"code","metadata":{"id":"BqFDMcbpLEzi"},"source":["!wget \"https://competitions.codalab.org/my/datasets/download/29a85805-2d8d-4701-a9ab-295180c89eb3\"\n","!unzip -q /content/29a85805-2d8d-4701-a9ab-295180c89eb3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYf0_yXq9qRO"},"source":["# DIRECTORIES DEFINITION"]},{"cell_type":"markdown","metadata":{"id":"2ixJuxZt9dnw"},"source":["## CROPPING FUNCTION"]},{"cell_type":"code","metadata":{"id":"BPA89FdF1p7Q"},"source":["import cv2\n","\n","def img_crop(img_source, img_dest, mask_source, mask_dest, name, frmt):\n","\n","  \"\"\"\n","  This function crops a specific image into 256x256 pieces. If the image is not\n","  divisible by 256, it will create some images with duplicated portions.\n","\n","  \"\"\"\n","\n","  DIM = 256\n","\n","  update_last_row = False\n","  update_last_column = False\n","\n","  img_arr = cv2.imread(img_source)\n","  mask_arr = cv2.imread(mask_source)\n","\n","  if (img_arr.shape[0]%DIM):\n","    update_last_row = True\n","  if (img_arr.shape[1]%DIM):\n","    update_last_column = True\n","\n","  # default crops\n","  for l in range(img_arr.shape[0]//DIM):\n","    for c in range(img_arr.shape[1]//DIM):\n","      # image crop & save\n","      crop_img_arr = img_arr[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(img_dest, name + \"_\" + str(l) + \"_\" + str(c) + frmt), crop_img_arr)\n","      # mask crop & save\n","      crop_mask_arr = mask_arr[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(mask_dest, name + \"_\" + str(l) + \"_\" + str(c) + '.png'), crop_mask_arr)\n","  \n","  # last smaller column crop (there will be some duplicated portions of images)\n","  if update_last_column:\n","    for l in range(img_arr.shape[0]//DIM):\n","      # image crop & save\n","      crop_img_arr = img_arr[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]]\n","      cv2.imwrite(os.path.join(img_dest, name + \"_\" + str(l) + \"_last\" + frmt), crop_img_arr)\n","      # mask crop & save\n","      crop_mask_arr = mask_arr[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]]\n","      cv2.imwrite(os.path.join(mask_dest, name + \"_\" + str(l) + \"_last\" + '.png'), crop_mask_arr)\n","\n","  # last smaller row crop (there will be some duplicated portions of images)\n","  if update_last_row:\n","    for c in range(img_arr.shape[1]//DIM):\n","      # image crop & save\n","      crop_img_arr = img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(img_dest, name + \"_last\" + \"_\" + str(c) + frmt), crop_img_arr)\n","      # mask crop & save\n","      crop_mask_arr = mask_arr[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM]\n","      cv2.imwrite(os.path.join(mask_dest, name + \"_last\" + \"_\" + str(c) + '.png'), crop_mask_arr)\n","\n","  # remaining portion crop (there will be some duplicated portions of images)\n","  if update_last_column and update_last_row:\n","    # image crop & save\n","    crop_img_arr = img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]]\n","    cv2.imwrite(os.path.join(img_dest, name + \"_last_last\" + frmt), crop_img_arr)\n","    # mask crop & save\n","    crop_mask_arr = mask_arr[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]]\n","    cv2.imwrite(os.path.join(mask_dest, name + \"_last_last\" + '.png'), crop_mask_arr)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8KudMc6lPSJH"},"source":["## FINAL DATASET\n","This script will create a folder called \"**FinalDataset**\" and a sub-folder called \"training\"."]},{"cell_type":"markdown","metadata":{"id":"KSW1nLJB6HUE"},"source":["### training dir\n","\n","Inside \"**training**\" there will be again 2 sub-folders: \"images\" and \"masks\". The first one will contain all the four-team images depicting both mais and haricot. On the other side, the second sub-directory will contain the respective masks of the pictures present in \"images\"."]},{"cell_type":"code","metadata":{"id":"n_nyHC5xRLg-"},"source":["import shutil\n","\n","dataset_dir = '/content/FinalDataset'\n","training_dir = os.path.join(dataset_dir, 'training')\n","\n","if not os.path.exists(dataset_dir):\n","  \n","  # FinalDataset\n","  os.makedirs(dataset_dir)\n","\n","  # training\n","  os.makedirs(training_dir)\n","  # images & masks\n","  os.makedirs(os.path.join(training_dir, 'images/img'))\n","  os.makedirs(os.path.join(training_dir, 'masks/img'))\n","\n","  starting_training_dir = '/content/Development_Dataset/Training'\n","  for team in os.listdir(starting_training_dir):\n","    for crop in os.listdir(os.path.join(starting_training_dir, team)):\n","      for f in os.listdir(os.path.join(starting_training_dir, team, crop, 'Images')):\n","        img_crop(img_source=os.path.join(starting_training_dir, team, crop, 'Images', f),\n","                 img_dest=os.path.join(training_dir, 'images/img'),\n","                 mask_source=os.path.join(starting_training_dir, team, crop, 'Masks', f[:-4] + '.png'),\n","                 mask_dest=os.path.join(training_dir, 'masks/img'),\n","                 name=f[:-4],\n","                 frmt=f[-4:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjKJGKL3hzOz"},"source":["### validation dir\n","This script will divide the whole content of \"/content/FinalDataset/training/images\" into 2 parts. The 80% of images into \"training\" and the 20% into \"**validation**\"."]},{"cell_type":"code","metadata":{"id":"m2KiZGl7iXE9"},"source":["import random, itertools \n","\n","validation_dir = '/content/FinalDataset/validation'\n","\n","if not os.path.exists(validation_dir):\n","  os.makedirs(validation_dir)\n","  os.makedirs(os.path.join(validation_dir, 'images/img'))\n","  os.makedirs(os.path.join(validation_dir, 'masks/img'))\n","else:\n","  # if already present the validation dir, this script will shuffle its content. \n","  # It will moove all the elements again to the \"training\" dir and after a shuffle \n","  # it will take 20% of the content and put ot in the \"validation\" dir.\n","  for f in os.listdir(os.path.join(validation_dir, 'images/img')):\n","      o = shutil.move(os.path.join(validation_dir, 'images/img', f),\n","                      os.path.join(training_dir, 'images/img', f))\n","      o = shutil.move(os.path.join(validation_dir, 'masks/img', f[:-3] + 'png'),\n","                      os.path.join(training_dir, 'masks/img', f[:-3] + 'png'))\n","      \n","    \n","images = os.listdir(os.path.join(training_dir, 'images/img'))\n","random.shuffle(images)\n","for f in itertools.islice(images, 0, int(0.2*len(images))):\n","  o = shutil.move(os.path.join(training_dir, 'images/img', f),\n","                  os.path.join(validation_dir, 'images/img', f))\n","  o = shutil.move(os.path.join(training_dir, 'masks/img', f[:-3] + 'png'),\n","                  os.path.join(validation_dir, 'masks/img', f[:-3] + 'png'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0z8WIzkDcQy"},"source":["!ls /content/FinalDataset/validation/img/images | wc -l\n","#!rm -rf ./FinalDataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7M50ZVsH40w2"},"source":["# GENERATORS"]},{"cell_type":"markdown","metadata":{"id":"FH6U4tVz43O8"},"source":["## IMAGE DATA GENERATOR"]},{"cell_type":"code","metadata":{"id":"0E9-cyNOEa1p"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","apply_data_augmentation = True\n","\n","if apply_data_augmentation:\n","    train_img_data_gen = ImageDataGenerator(rotation_range=30,\n","                                            width_shift_range=10,\n","                                            height_shift_range=10,\n","                                            zoom_range=0.3,\n","                                            horizontal_flip=True,\n","                                            vertical_flip=True,\n","                                            fill_mode='reflect',\n","                                            rescale=1./255)\n","    train_mask_data_gen = ImageDataGenerator(rotation_range=30,\n","                                             width_shift_range=10,\n","                                             height_shift_range=10,\n","                                             zoom_range=0.3,\n","                                             horizontal_flip=True,\n","                                             vertical_flip=True,\n","                                             fill_mode='reflect')\n","else:\n","    train_img_data_gen = ImageDataGenerator(rescale=1./255)\n","    train_mask_data_gen = ImageDataGenerator()\n","    \n","valid_img_data_gen = ImageDataGenerator(rescale=1./255)\n","valid_mask_data_gen = ImageDataGenerator()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6Ojd2Sm48sb"},"source":["## FLOW FROM DIRECTORY"]},{"cell_type":"code","metadata":{"id":"CdBHzPGEujOv"},"source":["# img shape\n","img_h = 256\n","img_w = 256\n","\n","# Batch size\n","bs = 64\n","\n","train_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n","                                                       target_size=(img_h, img_w),\n","                                                       batch_size=bs,\n","                                                       class_mode=None, \n","                                                       shuffle=True,\n","                                                       interpolation='bilinear',\n","                                                       seed=SEED) \n","\n","train_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n","                                                         target_size=(img_h, img_w),\n","                                                         batch_size=bs,\n","                                                         class_mode=None,\n","                                                         shuffle=True,\n","                                                         interpolation='bilinear',\n","                                                         seed=SEED)\n","train_gen = zip(train_img_gen, train_mask_gen)\n","\n","valid_img_gen = valid_img_data_gen.flow_from_directory(os.path.join(validation_dir, 'images'),\n","                                                       target_size=(img_h, img_w),\n","                                                       batch_size=bs, \n","                                                       class_mode=None, \n","                                                       shuffle=False,\n","                                                       interpolation='bilinear',\n","                                                       seed=SEED)\n","valid_mask_gen = valid_mask_data_gen.flow_from_directory(os.path.join(validation_dir, 'masks'),\n","                                                         target_size=(img_h, img_w),\n","                                                         batch_size=bs, \n","                                                         class_mode=None,\n","                                                         shuffle=False,\n","                                                         interpolation='bilinear',\n","                                                         seed=SEED)\n","valid_gen = zip(valid_img_gen, valid_mask_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lafViE0O5ASV"},"source":["## PREPARE TARGET FUNCTION\n","Questa funzione serve per fare in modo che le maschere non siano poi immagini RGB ma immagini a singolo canale con i target corrispondenti alle classi del problema."]},{"cell_type":"code","metadata":{"id":"sH0rs_V3nIZ0"},"source":["def prepare_target(x_, y_):\n","  # crop\n","  y_1 = tf.where(tf.reduce_any(y_ == [255, 255, 255], axis=-1, keepdims=True), 1, 0)\n","  print(y_1.shape)\n","  # weed\n","  y_2 = tf.where(tf.reduce_any(y_ == [216, 67, 82], axis=-1, keepdims=True), 2, 0)\n","  return x_, tf.cast(y_1+y_2, tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5K4UvvHe5OiW"},"source":["## DATASET DEFINITION"]},{"cell_type":"code","metadata":{"id":"U1iyv5rCE9k5"},"source":["train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 3]))\n","train_dataset = train_dataset.map(prepare_target)\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 3]))\n","valid_dataset = valid_dataset.map(prepare_target)\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NxcCZQeSbhn"},"source":["## DATA GENERATOR TEST"]},{"cell_type":"code","metadata":{"id":"bHAj3I8BRo3k"},"source":["iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WRxgjDvnQQd"},"source":["import time\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","# Assign a color to each class\n","colors_dict = {}\n","colors_dict[1] = [255, 255, 255]  # crop\n","colors_dict[0] = [0, 0, 0]  # background\n","colors_dict[2] = [216, 67, 82] # weed\n","\n","fig, ax = plt.subplots(1, 2)\n","\n","augmented_img, target = next(iterator)\n","augmented_img = augmented_img[0]   # First element\n","augmented_img = augmented_img * 255  # denormalize\n","augmented_img.shape\n","\n","target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n","\n","print(np.unique(target))\n","\n","# Assign colors (just for visualization)\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\n","\n","target_img[np.where(target == 0)] = colors_dict[0]\n","target_img[np.where(target == 1)] = colors_dict[1]\n","target_img[np.where(target == 2)] = colors_dict[2]\n","\n","ax[0].imshow(np.uint8(augmented_img))\n","ax[1].imshow(np.uint8(target_img))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ulx88lVm5sla"},"source":["# MODEL DEFINITION"]},{"cell_type":"code","metadata":{"id":"l8TvfNuvpt20"},"source":["vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n","vgg.summary()\n","for layer in vgg.layers:\n","  layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3vSaJElpppn"},"source":["def create_model(depth, start_f, num_classes):\n","\n","    model = tf.keras.Sequential()\n","    \n","    # Encoder\n","    # -------\n","    model.add(vgg)\n","    \n","    start_f = 256\n","        \n","    # Decoder\n","    # -------\n","    for i in range(depth):\n","        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n","        model.add(tf.keras.layers.Conv2D(filters=start_f,\n","                                         kernel_size=(3, 3),\n","                                         strides=(1, 1),\n","                                         padding='same'))\n","        model.add(tf.keras.layers.ReLU())\n","\n","        start_f = start_f // 2\n","\n","    # Prediction Layer\n","    # ----------------\n","    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n","                                     kernel_size=(1, 1),\n","                                     strides=(1, 1),\n","                                     padding='same',\n","                                     activation='softmax'))\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tP0Yws_gp6zS"},"source":["model = create_model(depth=5, \n","                     start_f=8, \n","                     num_classes=3)\n","\n","# Visualize created model as a table\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGzh16WTnFJW"},"source":["# PARAMS"]},{"cell_type":"code","metadata":{"id":"9MlmYGVMnFJW"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Here we define the intersection over union for each class in the batch.\n","# Then we compute the final iou as the mean over classes\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","# Validation metrics\n","# ------------------\n","metrics = ['accuracy', meanIoU]\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUPlIkmg53Wo"},"source":["# CALLBACKS"]},{"cell_type":"code","metadata":{"id":"uxQp3voMq7Gn"},"source":["from datetime import datetime\n","\n","cwd = '/content/drive/MyDrive/'\n","\n","experiments_dir = os.path.join(cwd, 'AN2DL_experiments_project_2')\n","if not os.path.exists(experiments_dir):\n","    os.makedirs(experiments_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","model_name = 'Proj2'\n","\n","proj_dir = os.path.join(experiments_dir, model_name + '_' + str(now))\n","if not os.path.exists(proj_dir):\n","    os.makedirs(proj_dir)\n","    \n","callbacks = []\n","\n","ckpt_dir = os.path.join(proj_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True) \n","callbacks.append(ckpt_callback)\n","\n","tb_dir = os.path.join(proj_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)\n","callbacks.append(tb_callback)\n","\n","early_stop = False\n","if early_stop:\n","    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0MZCPu9Q543l"},"source":["# TRAINING"]},{"cell_type":"code","metadata":{"id":"Yvbr57uJsC8r"},"source":["model.fit(x=train_dataset,\n","          epochs=100,  #### set repeat in training dataset\n","          steps_per_epoch=len(train_img_gen),\n","          validation_data=valid_dataset,\n","          validation_steps=len(valid_img_gen), \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFccIHClgD6H"},"source":["# SUBMISSION"]},{"cell_type":"markdown","metadata":{"id":"-UBoZZcBgIMS"},"source":["## MODEL RELOADING"]},{"cell_type":"code","metadata":{"id":"7Q31i5NUOnwn"},"source":["model.load_weights(\"/content/drive/MyDrive/AN2DL_experiments_project_2/Proj2_Dec12_14-58-22/ckpts/cp_08.ckpt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzFbUfC8gLYX"},"source":["## RLE ENCODE SCRIPT"]},{"cell_type":"code","metadata":{"id":"Fyc1XbHgZJXp"},"source":["def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHtiMzTrgUz-"},"source":["## SUBMISSION JSON"]},{"cell_type":"code","metadata":{"id":"fmVxjkraM_R8"},"source":["def predict_cropped_image(cropped_img):\n","  out_sigmoid = model.predict(x=tf.expand_dims(cropped_img, 0))\n","  predicted_class = tf.argmax(out_sigmoid, -1)\n","  return np.squeeze(predicted_class.numpy())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SxFv1NC85W-"},"source":["import json\n","\n","testing_dir = '/content/Development_Dataset/Test_Dev'\n","submission_dict = {}\n","\n","DIM = 256\n","\n","for team in os.listdir(testing_dir):\n","  update_last_row = False\n","  update_last_column = False\n","  for crop in os.listdir(os.path.join(testing_dir, team)):\n","      for f in os.listdir(os.path.join(testing_dir, team, crop, 'Images')):\n","\n","        img_arr = cv2.imread(os.path.join(testing_dir, team, crop, 'Images', f))\n","        \n","        # necessary due to cv2.imread that creates a numpy array \n","        # of the BGR image and not of the RGB one\n","        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n","\n","        if not update_last_row and img_arr.shape[0]%DIM:\n","          update_last_row = True\n","        if not update_last_column and img_arr.shape[1]%DIM:\n","          update_last_column = True       \n","\n","        # predictions di default\n","        total_prediction = np.zeros(img_arr.shape[0:2])\n","        for l in range(img_arr.shape[0]//DIM):\n","          for c in range(img_arr.shape[1]//DIM):\n","            total_prediction[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM] = predict_cropped_image(img_arr[l*DIM:l*DIM+DIM, c*DIM:c*DIM+DIM])\n","\n","        # predictions dell'ultima colonna\n","        if update_last_column:\n","          for l in range(img_arr.shape[0]//DIM):\n","            total_prediction[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]] = predict_cropped_image(img_arr[l*DIM:l*DIM+DIM, img_arr.shape[1]-DIM:img_arr.shape[1]])\n","\n","        # predictions dell'ultima riga\n","        if update_last_row:\n","          for c in range(img_arr.shape[1]//DIM):\n","            total_prediction[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM] = predict_cropped_image(img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], c*DIM:c*DIM+DIM])\n","\n","        # prediction cella rimanente\n","        if update_last_column and update_last_row:\n","          total_prediction[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]] = predict_cropped_image(img_arr[img_arr.shape[0]-DIM:img_arr.shape[0], img_arr.shape[1]-DIM:img_arr.shape[1]])\n","\n","        img_name = f[:-4]\n","\n","        submission_dict[img_name] = {}\n","        submission_dict[img_name]['shape'] = total_prediction.shape\n","        submission_dict[img_name]['team'] = team\n","        submission_dict[img_name]['crop'] = crop\n","        submission_dict[img_name]['segmentation'] = {}\n","\n","        # RLE encoding\n","        # crop\n","        rle_encoded_crop = rle_encode(total_prediction == 1)\n","        # weed\n","        rle_encoded_weed = rle_encode(total_prediction == 2)\n","\n","        submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","        submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","\n","with open(os.path.join('/content/submission.json'), 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}