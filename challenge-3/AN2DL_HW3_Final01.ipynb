{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW3_Final01.ipynb","provenance":[],"authorship_tag":"ABX9TyMYqiZUxn2qLjPUrf2q84bI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"42AusyTBzJBh"},"source":["# UTILITIES"]},{"cell_type":"markdown","metadata":{"id":"bY-_6tIIzMbd"},"source":["## SHELL OUTPUT"]},{"cell_type":"code","metadata":{"id":"6vVrC1FBy6CJ"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGXSYZhlzPWI"},"source":["## MODULES IMPORT"]},{"cell_type":"code","metadata":{"id":"_RgDt6zezQ2Q"},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import pickle\n","import zipfile\n","import json\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6CRSV7_jzaxK"},"source":["## UNZIP DATA"]},{"cell_type":"code","metadata":{"id":"D6NiFyc1zcH0"},"source":["# since there will be some operations on disk, let's define a utilities directory where \n","# we will store matrixes, archives, etc.\n","utils_dir = '/content/drive/MyDrive/AN2DL-competitions/HW3'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Qr9aTUdkXjN"},"source":["# if the dataset directory still does not exist let's create it\n","# necessary to have the zip file of the dataset (downloadable from Kaggle)\n","# in the utils_directory defined above\n","if not os.path.exists('/content/VQA_Dataset'):\n","  os.makedirs('/content/VQA_Dataset')\n","  with zipfile.ZipFile(os.path.join(utils_dir, 'VQA_Dataset.zip'), 'r') as zip_ref:\n","    zip_ref.extractall('/content/VQA_Dataset')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnIzY3kSm_EW"},"source":["# necessary to have the zip file of the GloVe embedding in the utils_directory defined above\n","# downloadable from http://nlp.stanford.edu/data/glove.42B.300d.zip\n","if not os.path.exists('/content/glove.42B.300d.txt'):\n","  with zipfile.ZipFile(os.path.join(utils_dir, 'glove.42B.300d.zip'), 'r') as zip_ref:\n","    zip_ref.extractall('/content/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqVsR5ZpzgKY"},"source":["## PARAMS"]},{"cell_type":"code","metadata":{"id":"O1lzkD3jzhm1"},"source":["# set dataset directory\n","dataset_dir = '/content/VQA_Dataset'\n","# set the path of the GloVe txt\n","glove_path = '/content/glove.42B.300d.txt'\n","# image size\n","IMG_H = 224\n","IMG_W = 224\n","# batch size\n","BS = 64\n","# number of epochs\n","EPOCHS = 30\n","# learning rate\n","LR = 5e-4\n","# seed\n","SEED = 1234\n","# early stopping\n","ES = False\n","# number of classes\n","NC = 58"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3YBTYprdA84H"},"source":["## ANSWERS DICTIONARY"]},{"cell_type":"code","metadata":{"id":"BMr7rNSH0jlo"},"source":["LABELS_DICT = {\n","  '0': 0,\n","  '1': 1,\n","  '2': 2,\n","  '3': 3,\n","  '4': 4,\n","  '5': 5,\n","  'apple': 6,\n","  'baseball': 7,\n","  'bench': 8,\n","  'bike': 9,\n","  'bird': 10,\n","  'black': 11,\n","  'blanket': 12,\n","  'blue': 13,\n","  'bone': 14,\n","  'book': 15,\n","  'boy': 16,\n","  'brown': 17,\n","  'cat': 18,\n","  'chair': 19,\n","  'couch': 20,\n","  'dog': 21,\n","  'floor': 22,\n","  'food': 23,\n","  'football': 24,\n","  'girl': 25,\n","  'grass': 26,\n","  'gray': 27,\n","  'green': 28,\n","  'left': 29,\n","  'log': 30,\n","  'man': 31,\n","  'monkey bars': 32,\n","  'no': 33,\n","  'nothing': 34,\n","  'orange': 35,\n","  'pie': 36,\n","  'plant': 37,\n","  'playing': 38,\n","  'red': 39,\n","  'right': 40,\n","  'rug': 41,\n","  'sandbox': 42,\n","  'sitting': 43,\n","  'sleeping': 44,\n","  'soccer': 45,\n","  'squirrel': 46,\n","  'standing': 47,\n","  'stool': 48,\n","  'sunny': 49,\n","  'table': 50,\n","  'tree': 51,\n","  'watermelon': 52,\n","  'white': 53,\n","  'wine': 54,\n","  'woman': 55,\n","  'yellow': 56,\n","  'yes': 57\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRwPD88C0iD6"},"source":["# GLOVE EMBEDDING FUNCTIONS & STRUCTURES\n","\n","Let's define the embedding matrix and the words' index (GloVe embedding) if not defined yet. If already defined, let's retrieve them from the disk memory (Google Drive)."]},{"cell_type":"code","metadata":{"id":"sdwyX_Tk0pRg"},"source":["if not os.path.exists(os.path.join(utils_dir, 'embedding_matrix.h5')) or not os.path.exists(os.path.join(utils_dir,'word_idx.pickle')):\n","\n","  embeddings = {}\n","  word_idx = {}\n","    \n","  with open(glove_path,'r') as f:\n","    for i, line in enumerate(f):\n","      values = line.split(' ')\n","      word = values[0]\n","      coefs = np.asarray(values[1:], dtype='float32')\n","      embeddings[word] = coefs\n","      word_idx[word] = i+1 # perché lo 0 sarà assegnato alle parole non trovate!\n","      # let's break the loop when we already have the 300K most frequent words\n","      # 0 to 299998 = 299999, bisogna poi aggiungere 1 per le parole non trovate\n","      if i==299998:\n","        break\n","\n","  num_words = len(word_idx)\n","  embedding_matrix = np.zeros((1+num_words, 300)) # DIM = num_words_dictionary x 300 (300 is the number of coefs)\n","\n","  for i, word in enumerate(word_idx.keys()):\n","    embedding_matrix[i+1] = embeddings[word]\n","\n","  # let's store the embedding_matrix\n","  with h5py.File(os.path.join(utils_dir,'embedding_matrix.h5'), 'w') as hf:\n","    hf.create_dataset('embedding_matrix', data=embedding_matrix)\n","\n","  # let's store the word indexes\n","  with open(os.path.join(utils_dir,'word_idx.pickle'),'wb') as f:\n","    pickle.dump(word_idx, f)\n","\n","else:\n","\n","  with h5py.File(os.path.join(utils_dir,'embedding_matrix.h5'),'r') as hf:\n","    data = hf.get('embedding_matrix')\n","    embedding_matrix = np.array(data)\n","\n","  with open(os.path.join(utils_dir,'word_idx.pickle'),'rb') as file:\n","    word_idx = pickle.load(file)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqZNIzBO2o7U"},"source":["# DATASET"]},{"cell_type":"markdown","metadata":{"id":"mYYZA4bsc7R6"},"source":["## FUNCTIONS & CLASS DEFINITION\n","Functions for obtaining both question and answer matrixes. Class for CustomDataset."]},{"cell_type":"code","metadata":{"id":"s3BEcgFeouG5"},"source":["from keras.preprocessing.sequence import pad_sequences\n","import nltk\n","from collections import defaultdict\n","from keras.preprocessing import image\n","from PIL import Image\n","\n","nltk.download('punkt')\n","\n","def get_question_matrix(df):\n","  questions = df[['question']].values.tolist()\n","  seq_list = []\n","  for question in questions:\n","    words = nltk.word_tokenize(question[0].lower().replace(\"?\", \"\"))\n","    seq = []\n","    for word in words:\n","      seq.append(word_idx.get(word, 0))\n","    seq_list.append(seq)\n","  question_matrix = pad_sequences(seq_list) # DIM = num_questions x num_words_in_longer_question\n","  return question_matrix\n","\n","def answer_to_onehot():\n","  answers = [k for k in LABELS_DICT.keys()]\n","  answer_to_onehot = {}\n","  for i, word in enumerate(answers):\n","    onehot = np.zeros(NC)\n","    onehot[i] = 1.0\n","    answer_to_onehot[word] = onehot\n","  return answer_to_onehot\n","\n","def get_answer_matrix(df, answer_to_onehot):\n","  answers = df[['answer']].values.tolist()\n","  answer_matrix = np.zeros((len(answers), NC))\n","  for i, answer in enumerate(answers):\n","    answer_matrix[i] = answer_to_onehot.get(answer[0].lower())\n","  return answer_matrix\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","\n","  def __init__(self, images_dir, images_names, question_matrix, answer_matrix, batch_size):\n","\n","    self.images_dir = images_dir\n","    self.images_names = images_names\n","    self.question_matrix = question_matrix\n","    self.answer_matrix = answer_matrix\n","    self.batch_size = batch_size\n","\n","  def __len__(self):\n","    return int(np.floor(len(self.images_names)/(self.batch_size)))\n","\n","  def __getitem__(self, index):\n","\n","    X = np.zeros((self.batch_size, IMG_H, IMG_W, 3))\n","\n","    for i in range(self.batch_size):\n","      img = image.load_img(os.path.join(self.images_dir, self.images_names[(index*self.batch_size)+i][0] + '.png'), target_size=(IMG_H, IMG_W))\n","      x = image.img_to_array(img)\n","      X[i,] = x\n","\n","    return [self.question_matrix[index*self.batch_size:(index*self.batch_size)+self.batch_size], X], self.answer_matrix[index*self.batch_size:(index*self.batch_size)+self.batch_size]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynFX2Gg1BvVt"},"source":["## TRAINING & VALIDATION DATASETS"]},{"cell_type":"code","metadata":{"id":"F47qEQj1qH7E"},"source":["from sklearn.utils import shuffle \n","\n","df = pd.read_json(os.path.join(dataset_dir,\"train_questions_annotations.json\")).transpose()\n","df = shuffle(df)\n","\n","train_df, val_df = np.split(df, [int(.8*len(df))])\n","\n","answer_to_onehot = answer_to_onehot()\n","\n","train_question_matrix = get_question_matrix(train_df)\n","train_answer_matrix = get_answer_matrix(train_df, answer_to_onehot)\n","\n","val_question_matrix = get_question_matrix(val_df)\n","val_answer_matrix = get_answer_matrix(val_df, answer_to_onehot)\n","\n","# since there is a shuffle and a padding operation, it could happen that\n","# the padding added on the 2 matrixes (train and val) is different\n","# --> different dimensions are not allowed from the model that wants a fixed\n","# input. We have to resize the matrixes in order to have equivalent dimensions\n","tqm = train_question_matrix.shape[1]\n","vqm = val_question_matrix.shape[1]\n","\n","if tqm < vqm:\n","  train_question_matrix = np.hstack([np.zeros((train_question_matrix.shape[0], vqm-tqm)), \n","                                     train_question_matrix])\n","elif vqm < tqm:\n","  val_question_matrix = np.hstack([np.zeros((val_question_matrix.shape[0], tqm-vqm)), \n","                                     val_question_matrix])\n","  \n","#print(train_question_matrix.shape)\n","#print(train_answer_matrix.shape)\n","#print(val_question_matrix.shape)\n","#print(val_answer_matrix.shape)\n","\n","dataset_training = CustomDataset(os.path.join(dataset_dir, 'Images'), \n","                                 train_df[['image_id']].values.tolist(),\n","                                 train_question_matrix,\n","                                 train_answer_matrix,\n","                                 BS)\n","\n","dataset_validation = CustomDataset(os.path.join(dataset_dir, 'Images'), \n","                                   val_df[['image_id']].values.tolist(),\n","                                   val_question_matrix,\n","                                   val_answer_matrix,\n","                                   BS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PSwajT_PDSvc"},"source":["## DATASET TEST"]},{"cell_type":"code","metadata":{"id":"GaJQ7sFkDVY0"},"source":["iterator = iter(dataset_training)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7USCzmODcqG"},"source":["[q, img], a = next(iterator)\n","print(q.shape)\n","print(img.shape)\n","print(a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_4rxiT_upZe"},"source":["## MODEL DEFINITION"]},{"cell_type":"code","metadata":{"id":"HSFRM8uOuosp"},"source":["from keras.models import Sequential, Model\n","from keras.layers import Dense, Embedding, LSTM, Reshape, Dropout, concatenate, Flatten, Input\n","from keras.utils import plot_model\n","from keras.applications import VGG19\n","\n","def img_model():\n","\n","  vgg = VGG19(weights='imagenet', \n","            include_top = True, \n","            input_shape=(IMG_H, IMG_W, 3))\n","  for layer in vgg.layers:\n","    layer.trainable = False\n","\n","  edit_vgg = Model(vgg.input, vgg.layers[-2].output)\n","\n","  model = Sequential()\n","  model.add(edit_vgg)\n","  model.add(Dense(1024, input_dim=4096, activation='relu'))\n","\n","  return model\n","\n","def question_model(embedding_matrix, seq_length):\n","  model = Sequential()\n","  model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n","      weights=[embedding_matrix], input_length=seq_length, trainable=False, mask_zero=True))\n","  model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_matrix.shape[1])))\n","  model.add(Dropout(0.5))\n","  model.add(LSTM(units=512, return_sequences=False))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(1024, activation='relu'))\n","\n","  return model\n","\n","image_input = Input(shape=(IMG_H, IMG_W, 3))\n","encoded_image = img_model()(image_input)\n","\n","question_input = Input(shape=(train_question_matrix.shape[1]))\n","encoded_question = question_model(embedding_matrix, train_question_matrix.shape[1])(question_input)\n","\n","merged = concatenate([encoded_question, encoded_image])\n","output = Dropout(0.3)(merged)\n","output = Dense(1024, activation='relu')(output)\n","output = Dropout(0.3)(output)\n","output = Dense(NC, activation='softmax')(output)\n","\n","model = Model(inputs=[question_input, image_input], outputs=output)\n","model.summary()\n","#plot_model(model, '/content/model1.png', True, expand_nested=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OuuTmmIfDHfm"},"source":["## OPTIMIZATION PARAMS"]},{"cell_type":"code","metadata":{"id":"CBQ2qXXzCwTj"},"source":["# Loss -> i'm using one-hot-encoding\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","\n","# Optimizer\n","optimizer = tf.keras.optimizers.RMSprop(learning_rate=LR, rho=0.9)\n","\n","# Metrics\n","metrics = ['accuracy']\n","\n","# Set random seed\n","tf.random.set_seed(SEED)\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_6bPs6cDJou"},"source":["## CALLBACKS"]},{"cell_type":"code","metadata":{"id":"MUPz6qQryO-T"},"source":["from datetime import datetime\n","\n","cwd = '/content/drive/MyDrive/AN2DL-competitions/HW3'\n","\n","experiments_dir = os.path.join(utils_dir, 'experiments-singl-model')\n","if not os.path.exists(experiments_dir):\n","    os.makedirs(experiments_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","model_name = 'single_proj'\n","\n","proj_dir = os.path.join(experiments_dir, model_name + '_' + str(now))\n","if not os.path.exists(proj_dir):\n","    os.makedirs(proj_dir)\n","    \n","callbacks = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0ifVzIOAekI"},"source":["### Model Checkpoint"]},{"cell_type":"code","metadata":{"id":"ZV1JlF_IylxL"},"source":["ckpt_dir = os.path.join(proj_dir, 'checkpoints')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68YQn34pAhMD"},"source":["### Tensorboard"]},{"cell_type":"code","metadata":{"id":"ourFYBaD0EGZ"},"source":["tb_dir = os.path.join(proj_dir, 'tensorboard-logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)\n","callbacks.append(tb_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVVP0cruAkAJ"},"source":["### Early Stopping"]},{"cell_type":"code","metadata":{"id":"v20t8vff0GBr"},"source":["if ES:\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n","  callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSrgSjTjEC5R"},"source":["## MODEL FIT"]},{"cell_type":"code","metadata":{"id":"g-24cO6kEAyT"},"source":["model.fit(dataset_training,\n","          epochs=EPOCHS,\n","          steps_per_epoch=len(dataset_training),\n","          validation_data=dataset_validation,\n","          validation_steps=len(dataset_validation), \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtBAKR_eCzqh"},"source":["# SUBMISSION"]},{"cell_type":"markdown","metadata":{"id":"io2xl29jDAgI"},"source":["## MODEL RELOADING"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Njs3bO8nsb9W","executionInfo":{"status":"ok","timestamp":1612080967845,"user_tz":-60,"elapsed":9470,"user":{"displayName":"Leonardo Mandruzzato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpP_4w-YyRfoPLwYpen1DWyZNKd5CWI8L-DKegq3g=s64","userId":"04202313098734066302"}},"outputId":"98018078-08aa-4444-fdba-17886226fd34"},"source":["# Cambiare in base a migliore modello !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","model.load_weights(\"/content/drive/MyDrive/HW3_Utilities/HW3_Experiments/Proj3_Jan31_00-38-05/Checkpoints/cp_15.ckpt\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0b93133a90>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"QxsyvM1WDDI_"},"source":["## TEST CUSTOM DATASET"]},{"cell_type":"code","metadata":{"id":"DsTDZXABDGbd"},"source":["class TestCustomDataset(tf.keras.utils.Sequence):\n","\n","  def __init__(self, images_dir, images_names, question_matrix):\n","\n","    self.images_dir = images_dir\n","    self.images_names = images_names\n","    self.question_matrix = question_matrix\n","\n","  def __len__(self):\n","    return len(self.images_names)\n","\n","  def __getitem__(self, index):\n","\n","    img = image.load_img(os.path.join(self.images_dir, self.images_names[index][0] + '.png'), target_size=(IMG_H, IMG_W))\n","    x = image.img_to_array(img)\n","    X = np.expand_dims(x, axis=0)\n","\n","    return [np.expand_dims(self.question_matrix[index], axis=0), X]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmw6-YujDMJN"},"source":["## TEST DATASET"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTnlsalNDxZG","executionInfo":{"status":"ok","timestamp":1612080974082,"user_tz":-60,"elapsed":2369,"user":{"displayName":"Leonardo Mandruzzato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpP_4w-YyRfoPLwYpen1DWyZNKd5CWI8L-DKegq3g=s64","userId":"04202313098734066302"}},"outputId":"6662fe17-f8af-4f35-e4c6-03de968c5063"},"source":["test_df = pd.read_json(os.path.join(dataset_dir,\"test_questions.json\")).transpose()\n","#test_df = shuffle(test_df)\n","\n","test_question_matrix = get_question_matrix(test_df)\n","\n","tqm = test_question_matrix.shape[1]\n","\n","if tqm < 23:\n","  test_question_matrix = np.hstack([np.zeros((test_question_matrix.shape[0], 23-tqm)), \n","                                     test_question_matrix])\n","\n","print(test_question_matrix.shape)\n","  \n","test_generator = TestCustomDataset(os.path.join(dataset_dir, 'Images'), \n","                                  test_df[['image_id']].values.tolist(),\n","                                  test_question_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(6372, 23)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DgwZg-UCDVeI"},"source":["## PREDICTIONS & CSV"]},{"cell_type":"code","metadata":{"id":"Vis7N2lSIyps"},"source":["from datetime import datetime\n","\n","def create_csv(results, results_dir='./'):\n","\n","    csv_fname = 'results_'\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')\n","\n","pred = model.predict(test_generator)\n","results = {}\n","for i in range(len(pred)):\n","  results[str(test_df.index[i])] = np.argmax(pred[i])\n","\n","create_csv(results)"],"execution_count":null,"outputs":[]}]}